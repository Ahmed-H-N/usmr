---
title: "<b>Week 6: The Linear Model</b>"
subtitle: "Univariate Statistics and Methodology using R"
author: "Martin Corley"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: "AY 2020-2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - xaringan-themer.css
      - mc_libs/tweaks.css
    nature:
      beforeInit: "mc_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
library(knitr)
library(tidyverse)
library(ggplot2)
source('R/pres_theme.R')
knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
source('R/myfuncs.R')
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

class: inverse, center, middle

# Part 1:  Correlation++

```{r recreate,include=FALSE}
library(faux)
set.seed(29)
dat <- rnorm_multi(n=50,
                   mu=c(0.1,650),
                   sd=c(.009,60),
                   r=.4,
                   varnames=c('BloodAlc','RT'))
d20 <- dat %>% sample_n(20) # FIXME
```
---
# Back on the Road

.pull-left[
```{r frecap,fig.asp=.6,echo=FALSE}
p1 <- dat %>% ggplot(aes(x=BloodAlc,y=RT)) +
  xlab("Blood Alcohol %/vol") + ylab("RT (ms)") +
  geom_point(size=3)
p1
  rd <- cor.test(dat$BloodAlc,dat$RT)
```

.center[
$r= `r rd$estimate`$, $p = `r rd$p.value`$
]
]
.pull-right[
![](lecture_6_files/img/playmo_police.jpg)
]
???
- is there anything more the police' data about blood alcohol and reaction time can tell us?

- so far we know they're correlated, but it would be good if we could something about _how bad_ blood alcohol is

  + how much worse does drinking more make things?
---
# Back on the Road (2)

.pull-left[
```{r frecap2,fig.asp=.6,echo=FALSE}
p1 + geom_smooth(method="lm")
m1 <- lm(RT~BloodAlc, data=dat)
```

.tc[
"for every extra 0.01% blood alcohol, reaction time slows down by around `r round(coef(m1)[2]*0.01,0)` ms"
]
]
.pull-right[
![](lecture_6_files/img/playmo_police.jpg)

]
???
- this kind of information is based on the assumption that the relationship between blood alcohol and RT is _linear_

- that is, that each additional unit of blood alcohol affects reaction time by the same amount


---
# The Only Equation You Will Ever Need

.br3.pa2.f2.white.bg-gray[
$$ \textrm{outcome}_i = (\textrm{model})_i + \textrm{error}_i $$
]

.center.pt2[
![:scale 25%](lecture_6_files/img/model_error.svg)
]

---
count: false
# The Only Equation You Will Ever Need

.br3.pa2.f2.white.bg-gray[
$$ \textrm{outcome}_i = (\textrm{model})_i + \textrm{error}_i $$
]

- to get any further, we need to make _assumptions_

- nature of the **model** .tr[
(linear)
]

- nature of the **errors** .tr[
(normal)
]

---
# A Linear Model

.flex.items-top[
.w-40.pa2[
$$ \color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i $$
$$ \color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \epsilon_i $$
.center[
so the linear model itself is...
]

$$ \hat{y}_i = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} $$
.center[
![:scale 30%](lecture_6_files/img/formula.svg)
]
]
.w-60[
&nbsp;
]]
---
count: false
# A Linear Model

.flex.items-top[
.w-40.pa2[
$$ \color{red}{\textrm{outcome}_i} = \color{blue}{(\textrm{model})_i} + \textrm{error}_i $$
$$ \color{red}{y_i} = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} + \epsilon_i $$
.center[
so the linear model itself is...
]

$$ \hat{y}_i = \color{blue}{b_0 \cdot{} 1 + b_1 \cdot{} x_i} $$
.center[
![:scale 30%](lecture_6_files/img/formula.svg)
]
]
.w-60.pa2[
```{r bb,echo=F, fig.asp=.6}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1) +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),arrow=arrow(ends="both",length=unit(.05,"native")),colour="blue") +
  geom_segment(aes(x=1,xend=2,y=f(1),yend=f(1)),linetype="dotted") +
  geom_segment(aes(x=2,y=f(1),xend=2,yend=f(2)),arrow=arrow(ends="both",length=unit(.05,"native")),colour="blue") +
  annotate("text",x=.6,y=2.5,label="b[0]~(intercept)",
           size=5,parse=TRUE) +
  annotate("text",x=2.6,y=7.5,label="b[1]~(slope)",
           size=5,parse=TRUE) +
    ggtitle(expression(paste(b[0]," = 5, ",b[1]," = 2")))

p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  theme(axis.title.y = element_text(colour = "blue"))
```

]]
---
# Take An Observation

.flex.items-top[
.w-40.pa2[

.br3.bg-gray.white.f2.pa2.tc[
x<sub>i</sub> = 1.2, y<sub>i</sub> = 9.9
]
```{r vals,include=F}
xX <-1.2
yY <- 9.9
```
 
$$ \hat{y}_i = b_0\cdot{}1 + b_1\cdot{}x_i = 7.4 $$ 
$$ y_i = \hat{y}_i + \epsilon_i = 7.4 + \color{red}{2.5} $$

]
.w-60.pa2[
```{r errplot,fig.asp=.6,echo=FALSE}
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="red") +
  annotate("text",.8,8.6,label=expression(paste(epsilon[i]," (error)")),colour="red",size=5)
```
]
]
???
- $\hat{y}_i$ is what the model _predicts_ for $x_i$

- $y_i$ is the actual value that was observed for $x_i$

- why would we care?

  + for one thing, the model can predict $\hat{y}$ for values of $x$ that we have never observed

---
# Simplifying the Data

.pull-left[
- **NB** we're doing this for illustrative purposes only

```{r simple}
ourDat <- dat %>% sample_n(20)
  # take 20 data points
```
]
.pull-right[
```{r plot1, fig.asp=.6, echo=F}
ourDat %>% ggplot(aes(x=BloodAlc,y=RT)) +
  geom_point(size=3)
```
]

---
count: false
# Simplifying the Data
.pull-left[
- **NB** we're doing this for illustrative purposes only

```{r simple1, eval=F}
ourDat <- dat %>% sample_n(20)
  # take 20 data points
```
```{r simple2}
ourDat <- ourDat %>% 
  mutate(BloodAlc = BloodAlc*100)
```

```{r modh,include=FALSE}
m1 <- lm(RT ~ BloodAlc,data=ourDat)
```

]
.pull-right[
```{r plot3, fig.asp=.6, echo=F}
ourDat %>% ggplot(aes(x=BloodAlc,y=RT)) +
  geom_point(size=3) +
  theme(axis.text.x = element_text(colour = "red"))
```
]
---
count: false
# Simplifying the Data
.pull-left[
- **NB** we're doing this for illustrative purposes only

```{r simple1a, eval=F}
ourDat <- dat %>% sample_n(20)
  # take 20 data points
```
```{r simple2a,eval=F}
ourDat <- ourDat %>% 
  mutate(BloodAlc = BloodAlc*100)
```
.center[
"for every extra unit blood alcohol, reaction time slows down by around `r round(coef(m1)[2],0)` ms"
]

]
.pull-right[
```{r plot2, fig.asp=.6, echo=F}
ourDat %>% ggplot(aes(x=BloodAlc,y=RT)) +
  geom_point(size=3) +
  geom_smooth(method="lm")

```
]

--

.center.f3.pt3[
but how can we evaluate our model?
]
---
# A Linear Model for 20 Drinkers

.center[
```{r mod,fig.asp=.55,echo=FALSE,fig.width=6}
ourDat <- ourDat %>% mutate(pred=predict(m1),mRT=mean(RT))
p1 <- ourDat %>% ggplot(aes(x=BloodAlc,y=RT,yend=pred))

pRes <- p1 + geom_segment(aes(xend=BloodAlc),linetype="dotted",colour="red",size=1) +
  geom_smooth(method="lm",se=FALSE) +
  geom_point(size=3)
pRes
```
]

- a linear model describes the **best-fit line** through the data

- minimises the error terms $\epsilon$ or **residuals**
---
# Total Sum of Squares

$$ \textrm{total SS}=\sum{(y - \bar{y})^2} $$

.pull-left[
- sum of squares between observed $y$ and mean $\bar{y}$

- represents the total amount of variance in the model

- how much does the observed data vary from a model which says "there is no effect of $x$" (**null model**)?
]
.pull-right[
```{r totss,echo=F,fig.asp=.6}
pTot <- p1 + geom_abline(intercept=mean(ourDat$RT),slope=0,size=1) +
  geom_segment(aes(yend=mean(mRT),xend=BloodAlc),linetype="dotted",colour="red",size=1) +
  geom_point(size=3)
pTot
```

]
---
# Residual Sum of Squares

$$ \textrm{residual SS} = \sum{(y - \hat{y})^2} $$
.pull-left[
- sum of squared differences between observed $y$ and predicted $\hat{y}$

- represents the unexplained variance in the model

- how much does the observed data vary from the existing model?
]
.pull-right[
```{r residp,echo=F,fig.asp=.6}
pRes
```
]
---
# Model Sum of Squares

$$ \textrm{model SS} = \sum{(\hat{y} - \bar{y})^2} $$

.pull-left[
- sum of squared differences between predicted $\hat{y}$ and mean $\bar{y}$

- represents the additional variance explained by the current model over the null model
]
.pull-right[
```{r modp,echo=F,fig.asp=.6}
pMod <- p1 + geom_segment(aes(y=mRT,xend=BloodAlc),colour="red",linetype="dotted",size=1) +
  geom_hline(yintercept = mean(ourDat$RT),size=1) +
  geom_smooth(method="lm",se=FALSE) +
  geom_point(size=3)
pMod
```

]

---
# Testing the Model: _R_<sup>2</sup>

.pull-left[
$$ R^2 = \frac{\textrm{model SS}}{\textrm{total SS}} = \frac{\sum{(\hat{y}-\bar{y})^2}}{\sum{(y-\bar{y})^2}} $$



"how much the model improves over the null"


.pt3[
- $0 \le R^2 \le 1$

- we want $R^2$ to be large

- for a single predictor, $\sqrt{R^2} = |r|$

]]

.pull-right[
```{r echo=F,fig.asp=0.8}
library(patchwork)
pMod/pTot
```

]
---
# Testing the Model: _F_

.pull-left[
$F$ ratio depends on **mean squares** ( $\textrm{MS}_x = \textrm{SS}_x/\textrm{df}_x$ )

$$ R^2 = \frac{\textrm{model MS}}{\textrm{residual MS}} = \frac{\sum{(\hat{y}-\bar{y})^2}}{\sum{(y-\hat{y})^2}} $$



"how much the model improves over chance"


.pt3[
- $0 < F$

- we want $F$ to be large

- significance of $F$ does not always equate to a large (or theoretically sensible) effect

]]

.pull-right[
```{r echo=F,fig.asp=0.8}
library(patchwork)
pMod/pRes
```

]
---
# Linear Models in R

```{r lm1, eval=F}
mod <- lm(RT ~ BloodAlc, data=ourDat)
```

---
count: false
# Linear Models in R

```{r lm2, highlight.output=c(17,18)}
mod <- lm(RT ~ BloodAlc, data=ourDat)
summary(mod)
```

---
count: false
# Linear Models in R

```{r lm3, highlight.output=c(11,12)}
<<lm2>>
```
---
class: inverse, center, middle, animated, bounceInUp

# End

---
# Acknowledgements

- icons by Diego Lavecchia from the [Noun Project](https://thenounproject.com/)