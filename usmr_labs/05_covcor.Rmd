---
title: "Covariance, Correlation, and Modelling"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
library(patchwork)
set.seed(017)
```

:::red
**Preliminaries**  

1. Open Rstudio, make sure you have the USMR project open, and create a new RMarkdown document (giving it a title for this week). 

:::


# Reading: Covariance & Correlation  

Our data for this walkthrough is from a (hypothetical) study on memory. Twenty participants studied passages of text (c500 words long), and were tested a week later. The testing phase presented participants with 100 statements about the text. They had to answer whether each statement was true or false, as well as rate their confidence in each answer (on a sliding scale from 0 to 100). The dataset contains, for each participant, the percentage of items correctly answered, and the average confidence rating. Participants' ages were also recorded.   

Let's take a look at the relationships between the percentage of items answered correctly (`recall_accuracy`) and participants' average self-rating of confidence in their answers (`recall_confidence`): 
```{r message=FALSE,warning=FALSE}
library(tidyverse)
library(patchwork)

recalldata <- read_csv("https://edin.ac/2wHhCej")

ggplot(recalldata, aes(x=recall_confidence, recall_accuracy))+
  geom_point() + 
ggplot(recalldata, aes(x=age, recall_accuracy))+
  geom_point()
```

These two relationships look quite different.  

+ For participants who tended to be more confident in their answers, the percentage of items they correctly answered tends to be higher.  
+ The older participants were, the lower the percentage of items they correctly answered tended to be.  

Which relationship should we be more confident in and why? 

Ideally, we would have some means of quantifying the strength and direction of these sorts of relationship. This is where we come to the two summary statistics which we can use to talk about the association between two numeric variables: __Covariance__ and __Correlation__.  

## Covariance

:::yellow

Covariance is the measure of how two variables vary together. 
It is the change in one variable associated with the change in another variable.   

For samples, covariance is calculated using the following formula:

$$\mathrm{cov}(x,y)=\frac{1}{n-1}\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})$$

where:

- $x$ and $y$ are two variables; e.g., `age` and `recall_accuracy`;
- $i$ denotes the observational unit, such that $x_i$ is value that the $x$ variable takes on the $i$th observational unit, and similarly for $y_i$;
- $n$ is the sample size.

**In R**  
We can calculate covariance in R using the `cov()` function.  
`cov()` can take two variables `cov(x = , y = )`.  

```{r}
cov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)
```

:::

`r optbegin("Optional: Manually calculating covariance", olabel=FALSE)`
1. Create 2 new columns in the memory recall data, one of which is the mean recall accuracy, and one which is the mean recall confidence. 
```{r}
recalldata <-
  recalldata %>% mutate(
    maccuracy = mean(recall_accuracy),
    mconfidence = mean(recall_confidence)
  )
```

2. Now create three new columns which are:

    i. recall accuracy minus the mean recall accuracy - this is the $(x_i - \bar{x})$ part.   
    ii. confidence minus the mean confidence - and this is the $(y_i - \bar{y})$ part.   
    iii. the product of i. and ii. - this is calculating $(x_i - \bar{x})$$(y_i - \bar{y})$.    

```{r}
recalldata <- 
  recalldata %>% 
    mutate(
      acc_minus_mean_acc = recall_accuracy - maccuracy,
      conf_minus_mean_conf = recall_confidence - mconfidence,
      prod_acc_conf = acc_minus_mean_acc * conf_minus_mean_conf
    )

recalldata
```

3. Finally, sum the products, and divide by $n-1$

```{r}
recalldata %>%
  summarise(
    prod_sum = sum(prod_acc_conf),
    n = n()
  )

2243.46 / (20-1)
```

Which is the same result as using `cov()`:
```{r}
cov(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)
```
`r optend()`

`r optbegin("Optional: Covariance explained visually", olabel=FALSE)`
```{r echo=FALSE, message=FALSE,warning=FALSE}
set.seed(7135)
tibble(x=runif(10,20,80),y=rnorm(10,160,10)) %>%
  mutate(y=y+x/2,
         coldir = ifelse( (x>mean(x) & y>mean(y)) | (x<mean(x) & y<mean(y)), "pos","neg")
  ) -> df

p1<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()
  
p2<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)+1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))
  
p3<-ggplot(df,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  annotate("text",x=mean(df$x)+7, y=202,label=expression(x[i]-bar("x")))+
  annotate("text",x=71.5, y=mean(df$y)+7,label=expression(y[i]-bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)


p4<-ggplot(df,aes(x=x,y=y))+
  geom_point(aes(col=coldir))+
  scale_color_manual("",values=c("black","red"))+
  theme_classic()+
  theme(legend.position = "none")+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color="tomato1", data = df)+
  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color="tomato1", data = df)+
  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color="tomato1",data = df)

p5<-ggplot(df,aes(x=x,y=y))+
  geom_point(aes(col=coldir))+
  scale_color_manual("",values=c("blue","red"))+
  theme_classic()+
  theme(legend.position = "none")+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df$x)-1, y=max(df$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df$x)+5, y=mean(df$y)+1,label=expr(bar("y")))+
  geom_segment(aes(x = mean(x), y = 200.9559, xend = 69.47989, yend = 200.9559), color="tomato1", data = df)+
  geom_segment(aes(x = 69.47989, y = mean(y), xend = 69.47989, yend = 200.9559), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 211.5757, xend = 56.36053, yend = 211.5757), color="tomato1", data = df)+
  geom_segment(aes(x = 56.36053, y = mean(y), xend = 56.36053, yend = 211.5757), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 180.5799, xend = 44.37031, yend = 180.5799), color="tomato1", data = df)+
  geom_segment(aes(x = 44.37031, y = mean(y), xend = 44.37031, yend = 180.5799), color="tomato1",data = df)+
  geom_segment(aes(x = mean(x), y = 194.4188, xend = 45.32536, yend = 194.4188), color="skyblue3", data = df)+
  geom_segment(aes(x = 45.32536, y = mean(y), xend = 45.32536, yend = 194.4188), color="skyblue3",data = df)+
  geom_segment(aes(x = mean(x), y = 182.6440, xend = 66.73541, yend = 182.6440), color="skyblue3", data = df)+
  geom_segment(aes(x = 66.73541, y = mean(y), xend = 66.73541, yend = 182.6440), color="skyblue3",data = df)
```

Consider the following scatterplot:  
```{r echo=FALSE, message=FALSE}
p1
```
<br>
Now let's superimpose a vertical dashed line at the mean of $x$ ($\bar{x}$) and a horizontal dashed line at the mean of $y$ ($\bar{y}$):
```{r echo=FALSE, message=FALSE, warning=FALSE}
p2
```
<br>
Now let's pick one of the points, call it $x_i$, and show $(x_{i}-\bar{x})$ and $(y_{i}-\bar{y})$.  
<br>
Notice that this makes a rectangle.  
<br>
As $(x_{i}-\bar{x})$ and $(y_{i}-\bar{y})$ are both positive values, their product -  $(x_{i}-\bar{x})(y_{i}-\bar{y})$ - is positive. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
p3
```
<br>
In fact, for all these points in red, the product $(x_{i}-\bar{x})(y_{i}-\bar{y})$ is positive (remember that a negative multiplied by a negative gives a positive): 
```{r echo=FALSE, message=FALSE, warning=FALSE}
p4
```
<br>
And for these points in blue, the product $(x_{i}-\bar{x})(y_{i}-\bar{y})$ is negative:  
```{r echo=FALSE, message=FALSE, warning=FALSE}
p5
```
<br>
Now take another look at the formula for covariance:  

$$\mathrm{cov}(x,y)=\frac{\sum_{i=1}^n (x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}$$
  
It is the sum of all these products divided by $n-1$. It is the average of the products! 
`r optend()`
  
## Correlation - $r$

:::yellow

You can think of correlation as a standardized covariance. It has a scale from negative one to one, on which the distance from zero indicates the strength of the relationship.  
Just like covariance, positive/negative values reflect the nature of the relationship.  

The __correlation coefficient__ is a standardised number which quantifies the strength and direction of the **linear** relationship between two variables. In a population it is denoted by $\rho$, and in a sample it is denoted by $r$.  
  
We can calculate $r$ using the following formula:  
$$
r_{(x,y)}=\frac{\mathrm{cov}(x,y)}{s_xs_y}
$$ 

We can actually rearrange this formula to show that the correlation is simply the covariance, but with the values $(x_i - \bar{x})$ divided by the standard deviation ($s_x$), and the values $(y_i - \bar{y})$ divided by $s_y$: 
$$
r_{(x,y)}=\frac{1}{n-1} \sum_{i=1}^n \left( \frac{x_{i}-\bar{x}}{s_x} \right) \left( \frac{y_{i}-\bar{y}}{s_y} \right)
$$
<br>
The correlation is the simply the covariance of **standardised** variables (variables expressed as the distance _in standard deviations_ from the mean).    

  
**Properties of correlation coefficients**

+ $-1 \leq r \leq 1$
+ The sign indicates the direction of association
  + _positive association_ ($r > 0$) means that values of one variable tend to be higher when values of the other variable are higher
  + _negative association_ ($r < 0$) means that values of one variable tend to be lower when values of the other variable are higher
  + _no linear association_ ($r \approx 0$) means that higher/lower values of one variable do not tend to occur with higher/lower values of the other variable 
+ The closer $r$ is to $\pm 1$, the stronger the linear association
+ $r$ has no units and does not depend on the units of measurement
+ The correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$

**In R**  
Just like R has a `cov()` function for calculating covariance, there is a `cor()` function for calculating correlation: 
```{r}
cor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)
```

:::

`r optbegin("Optional: Manually calculating correlation", olabel=FALSE)`
We calculated above that $\mathrm{cov}(\texttt{recall_accuracy}, \texttt{recall_confidence})$ = `r cov(recalldata$recall_accuracy, recalldata$recall_confidence) %>% round(3)`.  
  
To calculate the _correlation_, we can simply divide this by the standard deviations of the two variables $s_{\texttt{recall_accuracy}} \times s_{\texttt{recall_confidence}}$

```{r}
recalldata %>% summarise(
  s_ra = sd(recall_accuracy),
  s_rc = sd(recall_confidence)
)

118.08 / (14.527 * 11.622)
```

Which is the same result as using `cor()`:
```{r}
cor(x = recalldata$recall_accuracy, y = recalldata$recall_confidence)
```
`r optend()`

## Correlation Test  

Now that we've seen the formulae for _covariance_ and _correlation_, as well as how to quickly calculate them in R using `cov()` and `cor()`, we can use a statistical test to establish the probability of finding an association this strong by chance alone.  

:::yellow 
__Hypotheses__ 

_Remember_, hypotheses are about the _population_ parameter (in this case the correlation between the two variables in the population - i.e., $\rho$).  

<div style="margin-left:15px">  
__Null Hypothesis__  

+ There is _not_ a linear relationship between $x$ and $y$ in the population.<br>$H_0: \rho = 0$  

<br>
__Alternative Hypothesis__  

i. There is a _positive_ linear relationship between $x$ and $y$ in the population.<br>$H_1: \rho > 0$  
ii. There is a _negative_ linear relationship between $x$ and $y$ in the population.<br>$H_1: \rho < 0$  
iii. There is a linear relationship between $x$ and $y$ in the population.<br>$H_1: \rho \neq 0$  
</div>

__Test statistic__ 

Our test statistic here is _another_ $t$ statistic, the formula for which depends on both the observed correlation ($r$) and the sample size ($n$):  

$$t = r \sqrt{\frac{n-2}{1-r^2}}$$

__$p$-value__

We calculate the p-value for our $t$-statistic as the long-run probability of a $t$-statistic with $n-2$ degrees of freedom being less than, greater than, or more extreme in either direction (depending on the direction of our alternative hypothesis) than our observed $t$-statistic.  

__Assumptions__ 

+ Both variables are quantitative
+ Both variables should be drawn from normally distributed populations.
+ The relationship between the two variables should be linear.  

**In R**  
We can test the significance of the correlation coefficient really easily with the function `cor.test()`:  

```{r}
cor.test(recalldata$recall_accuracy, recalldata$recall_confidence)
```
:::

`r optbegin("Optional: Manually conducting the correlation test", olabel=FALSE)`

Or, if we want to calculate our test statistic manually: 
```{r}
#calculate r
r = cor(recalldata$recall_accuracy, recalldata$recall_confidence)

#get n
n = nrow(recalldata)

#calculate t    
tstat = r * sqrt((n - 2) / (1 - r^2))

#calculate p-value for t, with df = n-2 
2*(1-pt(tstat, df=n-2))
```
`r optend()`

## Cautions! {-}  

Correlation is an invaluable tool for quantifying relationships between variables, but __must be used with care__.  

Below are a few things to be aware of when we talk about correlation. 

`r optbegin("Correlation can be heavily affected by outliers. Always plot your data!", olabel=FALSE)`  

The two plots below only differ with respect to the inclusion of _one_ observation. However, the correlation coefficient for the two sets of observations is markedly different.  
```{r echo=FALSE,message=FALSE,warning=FALSE}
df2<-df
df2[2,1]<-180

pp1 <- ggplot(df2,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  annotate("text",x=mean(df2$x)+1.5, y=max(df2$y)-5,label=expr(bar("x")))+
  geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  annotate("text",x=min(df2$x)+5, y=mean(df2$y)+1,label=expr(bar("y")))+
  geom_segment(aes(x = mean(x), y = df2[[2,2]], xend = df2[[2,1]], yend = df2[[2,2]]), color="skyblue3", data = df2)+
  geom_segment(aes(x = df2[[2,1]], y = mean(y), xend = df2[[2,1]], yend = df2[[2,2]]), color="skyblue3",data = df2)+
  labs(title=paste0("Cov = ",cov(df2[,1],df2[,2]) %>% round(2),"   r = ",cor(df2[,1],df2[,2]) %>% round(2)))+
  xlim(35,185)

df3<-df2[-2,]
pp2 <- ggplot(df3,aes(x=x,y=y))+
  geom_point()+
  theme_classic()+
  #geom_vline(aes(xintercept=mean(x)), lty="dashed")+
  #annotate("text",x=mean(df3$x)+1.5, y=max(df3$y)-5,label=expr(bar("x")))+
  #geom_hline(aes(yintercept=mean(y)), lty="dashed")+
  #annotate("text",x=min(df3$x)+5, y=mean(df3$y)+1,label=expr(bar("y")))+
  #geom_segment(aes(x = mean(x), y = df3[[2,2]], xend = df3[[2,1]], yend = df3[[2,2]]), color="skyblue3", data = df3)+
  #geom_segment(aes(x = df3[[2,1]], y = mean(y), xend = df3[[2,1]], yend = df3[[2,2]]), color="skyblue3",data = df3)+
  labs(title=paste0("Cov = ",cov(df3[,1],df3[,2]) %>% round(2),"   r = ",cor(df3[,1],df3[,2]) %>% round(2)))+
  xlim(35,185)

pp2 / pp1
```
`r optend()`
`r optbegin("r = 0 means no linear association. The variables could still be otherwise associated. Always plot your data!", olabel = FALSE)`
The correlation coefficient in Figure \@ref(fig:joface) below is negligible, suggesting no _linear_ association. The word "linear" here is crucial - the data are very clearly related. 

```{r joface, echo=FALSE,message=FALSE,warning=FALSE, fig.cap="Unrelated data?"}
faced<-read_csv("data/face.csv")

ggplot(faced,aes(x=lm_x,y=lm_y))+
  geom_point()+
  theme_classic()+
  xlim(-3,3)+
  labs(title=paste0("Cov = ",cov(faced$lm_x,faced$lm_y) %>% round(2),"   r = ",cor(faced$lm_x,faced$lm_y) %>% round(2)))
```

Similarly, take look at all the sets of data in Figure \@ref(fig:datasaurus) below. The summary statistics (means and standard deviations of each variable, and the correlation) are almost identical, but the visualisations suggest that the data are very different from one another.
```{r datasaurus, echo=FALSE, fig.cap="Datasaurus! "}
knitr::include_graphics("https://media1.giphy.com/media/UN2kVJQeMFUje/source.gif")
```
__Source:__ Matejka, J., & Fitzmaurice, G. (2017, May). Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing. In _Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems_ (pp. 1290-1294).  
`r optend()`

`r optbegin("Correlation does not imply causation!", olabel=FALSE)`

```{r xkcdcor, echo=FALSE, fig.cap="https://imgs.xkcd.com/comics/correlation.png"}
knitr::include_graphics("https://imgs.xkcd.com/comics/correlation.png")
```

You will have likely heard the phrase "correlation does not imply causation". There is even a whole [wikipedia entry](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation) devoted to the topic.  

__Just because you observe an association between x and y, we should not deduce that x causes y__

An often cited paper (See Figure \@ref(fig:choco)) which appears to fall foul of this error took a correlation between a country's chocolate consumption and its number of nobel prize winners to suggest a _causal relationship_ between the two:  

```{r choco,echo=FALSE,warning=FALSE,message=FALSE, fig.cap="Chocolate consumption causes more Nobel Laureates?"}
knitr::include_graphics("https://pbs.twimg.com/media/D0AS2iEX0AAuMLX.jpg")
```

_"since chocolate consumption has been documented to improve cognitive function, it seems most likely that in a dose-dependent way, chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates"_    
[Messerli, Franz. Chocolate Consumption, Cognitive Function, and Nobel Laureates. The New England Journal of Medicine 2012; 367:1562-4, (http://www.nejm.org/doi/full/10.1056/NEJMon1211064)]
`r optend()`

# <b>Game:</b> Guess the $r$ {-}
Take a break and play this "guess the correlation" game to get an idea of what different strengths and directions of $r$ can look like.   
(if the game is not showing, try http://guessthecorrelation.com/). 
  
`r knitr::include_url("http://guessthecorrelation.com/", height="650px")`
**source: [http://guessthecorrelation.com/](http://guessthecorrelation.com/)**

# Exercises:

# Reading: functions and models

# Exercsies: 